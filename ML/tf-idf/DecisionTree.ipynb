{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shreygupta/Documents/Classes/CS598DLH'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/Users/shreygupta/Documents/Classes/CS598DLH/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:weka.core.jvm:Adding bundled jars\n",
      "DEBUG:weka.core.jvm:Classpath=['/Users/shreygupta/Documents/Classes/CS598DLH/.conda/lib/python3.10/site-packages/javabridge/jars/rhino-1.7R4.jar', '/Users/shreygupta/Documents/Classes/CS598DLH/.conda/lib/python3.10/site-packages/javabridge/jars/runnablequeue.jar', '/Users/shreygupta/Documents/Classes/CS598DLH/.conda/lib/python3.10/site-packages/javabridge/jars/cpython.jar', '/Users/shreygupta/Documents/Classes/CS598DLH/.conda/lib/python3.10/site-packages/weka/lib/python-weka-wrapper.jar', '/Users/shreygupta/Documents/Classes/CS598DLH/.conda/lib/python3.10/site-packages/weka/lib/weka.jar']\n",
      "DEBUG:weka.core.jvm:MaxHeapSize=default\n",
      "DEBUG:weka.core.jvm:Package support disabled\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shreygupta/Documents/Classes/CS598DLH/.conda/lib/python3.10/site-packages/javabridge/jutil.py\", line 282, in start_thread\n",
      "    vm.create_mac(args, RQCLS, library_path, libjli_path)\n",
      "  File \"_javabridge.pyx\", line 709, in _javabridge.JB_VM.create_mac\n",
      "RuntimeError: Failed to create Java VM. Return code = -1\n",
      "ERROR:javabridge.jutil:Failed to create Java VM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open libjli.dylib.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to start Java VM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mweka\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassifiers\u001b[39;00m \u001b[39mimport\u001b[39;00m Classifier, Evaluation\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimblearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mover_sampling\u001b[39;00m \u001b[39mimport\u001b[39;00m SMOTE\n\u001b[0;32m---> 18\u001b[0m jvm\u001b[39m.\u001b[39;49mstart()\n",
      "File \u001b[0;32m~/Documents/Classes/CS598DLH/.conda/lib/python3.10/site-packages/weka/core/jvm.py:145\u001b[0m, in \u001b[0;36mstart\u001b[0;34m(class_path, bundled, packages, system_cp, max_heap_size, system_info, auto_install, logging_level)\u001b[0m\n\u001b[1;32m    142\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAutomatically installing missing Weka packages (based on suggestions).\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m     automatically_install_packages \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m javabridge\u001b[39m.\u001b[39;49mstart_vm(args\u001b[39m=\u001b[39;49margs, run_headless\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_heap_size\u001b[39m=\u001b[39;49mmax_heap_size)\n\u001b[1;32m    146\u001b[0m javabridge\u001b[39m.\u001b[39mattach()\n\u001b[1;32m    147\u001b[0m started \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Classes/CS598DLH/.conda/lib/python3.10/site-packages/javabridge/jutil.py:319\u001b[0m, in \u001b[0;36mstart_vm\u001b[0;34m(args, class_path, max_heap_size, run_headless)\u001b[0m\n\u001b[1;32m    317\u001b[0m start_event\u001b[39m.\u001b[39mwait()\n\u001b[1;32m    318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _javabridge\u001b[39m.\u001b[39mget_vm()\u001b[39m.\u001b[39mis_active():\n\u001b[0;32m--> 319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFailed to start Java VM\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m attach()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to start Java VM"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import collections\n",
    "from feature_generation import FeatureGeneration\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import weka.core.jvm as jvm\n",
    "from weka.core.converters import Loader\n",
    "from weka.filters import Filter\n",
    "from weka.attribute_selection import ASEvaluation, AttributeSelection\n",
    "from weka.classifiers import Classifier, Evaluation\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "jvm.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, x_train, y_train, x_test, y_test, k):\n",
    "        self.dtc = DecisionTreeClassifier(splitter='random', random_state=42)\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.k = k\n",
    "        \n",
    "    def feature_selection_SelectKBest(self):\n",
    "        k_best = SelectKBest(chi2, k=self.k)\n",
    "        k_best.fit(self.x_train, self.y_train)\n",
    "        self.x_train = k_best.transform(self.x_train)\n",
    "        self.x_test = k_best.transform(self.x_test)\n",
    "        \n",
    "    def feature_selection_ExtraTreesClassifier(self):\n",
    "        clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "        clf.fit(self.x_train, self.y_train)\n",
    "        importances = clf.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        self.x_train = self.x_train[:, indices[:self.k]]\n",
    "        self.x_test = self.x_test[:, indices[:self.k]]\n",
    "\n",
    "    def feature_selection_InfoGainAttributeEval(self, morbidity):\n",
    "        loader = Loader(classname=\"weka.core.converters.ArffLoader\")\n",
    "        train_data = loader.load_file(f\"./dataset/train/train_{morbidity}_tfidf.arff\")\n",
    "        train_data.class_is_last()\n",
    "\n",
    "        # Initialize attribute selection\n",
    "        eval = ASEvaluation(classname=\"weka.attributeSelection.InfoGainAttributeEval\")\n",
    "        search = AttributeSelection()\n",
    "        search.evaluator = eval\n",
    "        search.select_attributes(train_data)\n",
    "        selected_attributes = search.selected_attributes\n",
    "        filtered_attributes = np.delete(selected_attributes, [-1])\n",
    "\n",
    "        # Apply selected attributes to the training and testing sets\n",
    "        self.x_train = self.x_train[:, filtered_attributes]\n",
    "        self.x_test = self.x_test[:, filtered_attributes]\n",
    "\n",
    "    def train(self):\n",
    "        self.dtc.fit(self.x_train, self.y_train)\n",
    "\n",
    "    def test_and_evaluate(self):\n",
    "        y_pred = self.dtc.predict(self.x_test)\n",
    "        f1_macro = f1_score(self.y_test, y_pred, average='macro')\n",
    "        f1_micro = f1_score(self.y_test, y_pred, average='micro')\n",
    "        return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "morbidities = ['Asthma', 'CAD', 'CHF', 'Depression', 'Diabetes', 'Gallstones', 'GERD', 'Gout', 'Hypercholesterolemia', 'Hypertension', 'Hypertriglyceridemia', 'OA', 'Obesity', 'OSA', 'PVD', 'Venous_Insufficiency']\n",
    "\n",
    "column_headings = [\"Morbidity Class\", \"DT_Macro F1\", \"DT_Micro F1\"]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(572, 600) (572,) Counter({0.0: 502, 1.0: 70})\n",
      "Asthma == Macro F1 score: 0.9546519505256145 and Micro F1 Score 0.9551683168316831\n",
      "(548, 600) (548,) Counter({1.0: 325, 0.0: 223})\n",
      "CAD == Macro F1 score: 0.8536166138521624 and Micro F1 Score 0.8553846153846155\n",
      "(243, 600) (243,) Counter({1.0: 243})\n",
      "CHF == Macro F1 score: 1 and Micro F1 Score 1\n",
      "(582, 600) (582,) Counter({0.0: 460, 1.0: 122})\n",
      "Depression == Macro F1 score: 0.8789546355205691 and Micro F1 Score 0.8804347826086957\n",
      "(567, 600) (567,) Counter({1.0: 396, 0.0: 171})\n",
      "Diabetes == Macro F1 score: 0.9415394268987483 and Micro F1 Score 0.9418987341772151\n",
      "(593, 600) (593,) Counter({0.0: 506, 1.0: 87})\n",
      "Gallstones == Macro F1 score: 0.8418077579457431 and Micro F1 Score 0.8438167346146379\n",
      "(487, 600) (487,) Counter({0.0: 372, 1.0: 115})\n",
      "GERD == Macro F1 score: 0.7426973329922422 and Micro F1 Score 0.7459639639639639\n",
      "(596, 600) (596,) Counter({0.0: 518, 1.0: 78})\n",
      "Gout == Macro F1 score: 0.8619235790717091 and Micro F1 Score 0.8639563106796118\n",
      "(502, 600) (502,) Counter({1.0: 262, 0.0: 240})\n",
      "Hypercholesterolemia == Macro F1 score: 0.7887266869489515 and Micro F1 Score 0.7937590711175617\n",
      "(531, 600) (531,) Counter({1.0: 428, 0.0: 103})\n",
      "Hypertension == Macro F1 score: 0.8911970333727945 and Micro F1 Score 0.8925444596443229\n",
      "(587, 600) (587,) Counter({0.0: 554, 1.0: 33})\n",
      "Hypertriglyceridemia == Macro F1 score: 0.945252977689446 and Micro F1 Score 0.9458804258804259\n",
      "(565, 600) (565,) Counter({0.0: 467, 1.0: 98})\n",
      "OA == Macro F1 score: 0.8196732468910367 and Micro F1 Score 0.822214596202242\n",
      "(553, 600) (553,) Counter({0.0: 314, 1.0: 239})\n",
      "Obesity == Macro F1 score: 0.9432760011252702 and Micro F1 Score 0.9442652329749104\n",
      "(590, 600) (590,) Counter({0.0: 506, 1.0: 84})\n",
      "OSA == Macro F1 score: 0.9406587928705015 and Micro F1 Score 0.9416715200931858\n",
      "(556, 600) (556,) Counter({0.0: 469, 1.0: 87})\n",
      "PVD == Macro F1 score: 0.9252121299875503 and Micro F1 Score 0.9275680622283231\n",
      "(526, 600) (526,) Counter({0.0: 482, 1.0: 44})\n",
      "Venous_Insufficiency == Macro F1 score: 0.9003260012570428 and Micro F1 Score 0.9014390034364259\n"
     ]
    }
   ],
   "source": [
    "with open(\"./results/tf-idf/performance_DT_AllFeatures.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([column_headings[0], column_headings[1], column_headings[2]])\n",
    "    \n",
    "all_f1_macro_scores = []\n",
    "all_f1_micro_scores = []\n",
    "\n",
    "for morbidity in morbidities:\n",
    "    train_preprocessed_df = pd.read_csv('./dataset/train/train_intuitive_preprocessed.csv')\n",
    "    train_preprocessed_df = train_preprocessed_df[train_preprocessed_df[morbidity].isin([1.0, 0.0])]\n",
    "\n",
    "    X, Y, words = FeatureGeneration(train_preprocessed_df, morbidity).tf_idf()\n",
    "\n",
    "    if len(collections.Counter(list(Y)).keys()) >=2:\n",
    "        smote = SMOTE(random_state=42,k_neighbors=2)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X, Y)\n",
    "        X, Y =  X_train_resampled, y_train_resampled\n",
    "\n",
    "        # add KFold cross validation\n",
    "        skf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "        f1_macro_list = []\n",
    "        f1_micro_list = []\n",
    "        for train_idx, val_idx in skf.split(X, Y):\n",
    "            X_train_fold, Y_train_fold = X[train_idx], Y[train_idx]\n",
    "            X_val_fold, Y_val_fold = X[val_idx], Y[val_idx]\n",
    "\n",
    "            # Training RF using TF-IDF Representation\n",
    "            dt_obj = DecisionTree(X_train_fold, Y_train_fold, X_val_fold, Y_val_fold, 0)\n",
    "            dt_obj.train()\n",
    "\n",
    "            f1_macro, f1_micro = dt_obj.test_and_evaluate()\n",
    "\n",
    "            f1_macro_list.append(f1_macro)\n",
    "            f1_micro_list.append(f1_micro)\n",
    "\n",
    "        f1_macro = np.mean(f1_macro_list)\n",
    "        f1_micro = np.mean(f1_micro_list)\n",
    "    else:\n",
    "        f1_macro = 1\n",
    "        f1_micro = 1\n",
    "    \n",
    "    print(f\"{morbidity} == Macro F1 score: {f1_macro} and Micro F1 Score {f1_micro}\")\n",
    "\n",
    "    row_heading = morbidity\n",
    "\n",
    "    # data to be written to the CSV file\n",
    "    data = [f1_macro, f1_micro]\n",
    "    all_f1_macro_scores.append(f1_macro)\n",
    "    all_f1_micro_scores.append(f1_micro)\n",
    "\n",
    "    with open(\"./results/tf-idf/performance_DT_AllFeatures.csv\", \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        row = [row_heading]\n",
    "        row.extend(data)\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "with open(\"./results/tf-idf/performance_DT_AllFeatures.csv\", \"a\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    row = [\"Average\"]\n",
    "    row.extend([sum(all_f1_macro_scores)/len(all_f1_macro_scores),  sum(all_f1_micro_scores)/len(all_f1_micro_scores) ])\n",
    "    writer.writerow(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select K Best k=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(572, 600) (572,) Counter({0.0: 502, 1.0: 70})\n",
      "Asthma == Macro F1 score: 0.9597104872709916 and Micro F1 Score 0.9601584158415843\n",
      "(548, 600) (548,) Counter({1.0: 325, 0.0: 223})\n",
      "CAD == Macro F1 score: 0.8757710466742681 and Micro F1 Score 0.876923076923077\n",
      "(243, 600) (243,) Counter({1.0: 243})\n",
      "CHF == Macro F1 score: 1 and Micro F1 Score 1\n",
      "(582, 600) (582,) Counter({0.0: 460, 1.0: 122})\n",
      "Depression == Macro F1 score: 0.8857780990267348 and Micro F1 Score 0.8869565217391304\n",
      "(567, 600) (567,) Counter({1.0: 396, 0.0: 171})\n",
      "Diabetes == Macro F1 score: 0.9442102840154296 and Micro F1 Score 0.9445727848101265\n",
      "(593, 600) (593,) Counter({0.0: 506, 1.0: 87})\n",
      "Gallstones == Macro F1 score: 0.8453967927481836 and Micro F1 Score 0.8478353717724714\n",
      "(487, 600) (487,) Counter({0.0: 372, 1.0: 115})\n",
      "GERD == Macro F1 score: 0.7575748844168186 and Micro F1 Score 0.7635675675675676\n",
      "(596, 600) (596,) Counter({0.0: 518, 1.0: 78})\n",
      "Gout == Macro F1 score: 0.875802855748414 and Micro F1 Score 0.8773898431665422\n",
      "(502, 600) (502,) Counter({1.0: 262, 0.0: 240})\n",
      "Hypercholesterolemia == Macro F1 score: 0.8287235177545668 and Micro F1 Score 0.8338171262699564\n",
      "(531, 600) (531,) Counter({1.0: 428, 0.0: 103})\n",
      "Hypertension == Macro F1 score: 0.9270237623140775 and Micro F1 Score 0.9277291381668947\n",
      "(587, 600) (587,) Counter({0.0: 554, 1.0: 33})\n",
      "Hypertriglyceridemia == Macro F1 score: 0.9362513173285096 and Micro F1 Score 0.9368632268632269\n",
      "(565, 600) (565,) Counter({0.0: 467, 1.0: 98})\n",
      "OA == Macro F1 score: 0.8281444919173604 and Micro F1 Score 0.8307366735300846\n",
      "(553, 600) (553,) Counter({0.0: 314, 1.0: 239})\n",
      "Obesity == Macro F1 score: 0.9494500459279754 and Micro F1 Score 0.9505888376856119\n",
      "(590, 600) (590,) Counter({0.0: 506, 1.0: 84})\n",
      "OSA == Macro F1 score: 0.9396224479248236 and Micro F1 Score 0.940691127936323\n",
      "(556, 600) (556,) Counter({0.0: 469, 1.0: 87})\n",
      "PVD == Macro F1 score: 0.930127586180815 and Micro F1 Score 0.9317776252573783\n",
      "(526, 600) (526,) Counter({0.0: 482, 1.0: 44})\n",
      "Venous_Insufficiency == Macro F1 score: 0.9283500714209497 and Micro F1 Score 0.9294351374570446\n"
     ]
    }
   ],
   "source": [
    "with open(\"./results/tf-idf/performance_DT_SelectKBest.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([column_headings[0], column_headings[1], column_headings[2]])\n",
    "    \n",
    "all_f1_macro_scores = []\n",
    "all_f1_micro_scores = []\n",
    "\n",
    "for morbidity in morbidities:\n",
    "    train_preprocessed_df = pd.read_csv('./dataset/train/train_intuitive_preprocessed.csv')\n",
    "    train_preprocessed_df = train_preprocessed_df[train_preprocessed_df[morbidity].isin([1.0, 0.0])]\n",
    "\n",
    "    X, Y, words = FeatureGeneration(train_preprocessed_df, morbidity).tf_idf()\n",
    "\n",
    "    if len(collections.Counter(list(Y)).keys()) >=2:\n",
    "        smote = SMOTE(random_state=42,k_neighbors=2)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X, Y)\n",
    "        X, Y =  X_train_resampled, y_train_resampled\n",
    "        \n",
    "        # add KFold cross validation\n",
    "        skf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "        f1_macro_list = []\n",
    "        f1_micro_list = []\n",
    "        for train_idx, val_idx in skf.split(X, Y):\n",
    "            X_train_fold, Y_train_fold = X[train_idx], Y[train_idx]\n",
    "            X_val_fold, Y_val_fold = X[val_idx], Y[val_idx]\n",
    "\n",
    "            # Training RF using TF-IDF Representation\n",
    "            dt_obj = DecisionTree(X_train_fold, Y_train_fold, X_val_fold, Y_val_fold, 100)\n",
    "            dt_obj.feature_selection_SelectKBest()\n",
    "            dt_obj.train()   \n",
    "\n",
    "            f1_macro, f1_micro = dt_obj.test_and_evaluate()\n",
    "\n",
    "            f1_macro_list.append(f1_macro)\n",
    "            f1_micro_list.append(f1_micro)\n",
    "\n",
    "        f1_macro = np.mean(f1_macro_list)\n",
    "        f1_micro = np.mean(f1_micro_list)\n",
    "    else:\n",
    "        f1_macro = 1\n",
    "        f1_micro = 1\n",
    "    \n",
    "    print(f\"{morbidity} == Macro F1 score: {f1_macro} and Micro F1 Score {f1_micro}\")\n",
    "\n",
    "    row_heading = morbidity\n",
    "\n",
    "    # data to be written to the CSV file\n",
    "    data = [f1_macro, f1_micro]\n",
    "    all_f1_macro_scores.append(f1_macro)\n",
    "    all_f1_micro_scores.append(f1_micro)\n",
    "\n",
    "    with open(\"./results/tf-idf/performance_DT_SelectKBest.csv\", \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        row = [row_heading]\n",
    "        row.extend(data)\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "with open(\"./results/tf-idf/performance_DT_SelectKBest.csv\", \"a\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    row = [\"Overall-Average\"]\n",
    "    row.extend([sum(all_f1_macro_scores)/len(all_f1_macro_scores),  sum(all_f1_micro_scores)/len(all_f1_micro_scores) ])\n",
    "    writer.writerow(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(572, 600) (572,) Counter({0.0: 502, 1.0: 70})\n",
      "Asthma == Macro F1 score: 0.9626619377382826 and Micro F1 Score 0.9631386138613861\n",
      "(548, 600) (548,) Counter({1.0: 325, 0.0: 223})\n",
      "CAD == Macro F1 score: 0.8571311193183095 and Micro F1 Score 0.8584615384615384\n",
      "(243, 600) (243,) Counter({1.0: 243})\n",
      "CHF == Macro F1 score: 1 and Micro F1 Score 1\n",
      "(582, 600) (582,) Counter({0.0: 460, 1.0: 122})\n",
      "Depression == Macro F1 score: 0.8489901794922758 and Micro F1 Score 0.851086956521739\n",
      "(567, 600) (567,) Counter({1.0: 396, 0.0: 171})\n",
      "Diabetes == Macro F1 score: 0.930195821551685 and Micro F1 Score 0.9305379746835442\n",
      "(593, 600) (593,) Counter({0.0: 506, 1.0: 87})\n",
      "Gallstones == Macro F1 score: 0.8472939805522086 and Micro F1 Score 0.8508056688021742\n",
      "(487, 600) (487,) Counter({0.0: 372, 1.0: 115})\n",
      "GERD == Macro F1 score: 0.7579833350518224 and Micro F1 Score 0.7595855855855855\n",
      "(596, 600) (596,) Counter({0.0: 518, 1.0: 78})\n",
      "Gout == Macro F1 score: 0.8689602147110123 and Micro F1 Score 0.870687079910381\n",
      "(502, 600) (502,) Counter({1.0: 262, 0.0: 240})\n",
      "Hypercholesterolemia == Macro F1 score: 0.8250149182257422 and Micro F1 Score 0.8283381712626996\n",
      "(531, 600) (531,) Counter({1.0: 428, 0.0: 103})\n",
      "Hypertension == Macro F1 score: 0.9098269313201026 and Micro F1 Score 0.9112585499316006\n",
      "(587, 600) (587,) Counter({0.0: 554, 1.0: 33})\n",
      "Hypertriglyceridemia == Macro F1 score: 0.9305721865665773 and Micro F1 Score 0.9314332514332515\n",
      "(565, 600) (565,) Counter({0.0: 467, 1.0: 98})\n",
      "OA == Macro F1 score: 0.8297249829058533 and Micro F1 Score 0.83185769846717\n",
      "(553, 600) (553,) Counter({0.0: 314, 1.0: 239})\n",
      "Obesity == Macro F1 score: 0.9515728223391029 and Micro F1 Score 0.9522017409114184\n",
      "(590, 600) (590,) Counter({0.0: 506, 1.0: 84})\n",
      "OSA == Macro F1 score: 0.9414621370135731 and Micro F1 Score 0.9426422053970102\n",
      "(556, 600) (556,) Counter({0.0: 469, 1.0: 87})\n",
      "PVD == Macro F1 score: 0.9337888242989683 and Micro F1 Score 0.9349805536490507\n",
      "(526, 600) (526,) Counter({0.0: 482, 1.0: 44})\n",
      "Venous_Insufficiency == Macro F1 score: 0.9298084457232074 and Micro F1 Score 0.9305090206185568\n"
     ]
    }
   ],
   "source": [
    "with open(\"./results/tf-idf/performance_DT_ExtraTreesClassifier.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([column_headings[0], column_headings[1], column_headings[2]])\n",
    "    \n",
    "all_f1_macro_scores = []\n",
    "all_f1_micro_scores = []\n",
    "\n",
    "for morbidity in morbidities:\n",
    "\n",
    "    train_preprocessed_df = pd.read_csv('./dataset/train/train_intuitive_preprocessed.csv')\n",
    "    train_preprocessed_df = train_preprocessed_df[train_preprocessed_df[morbidity].isin([1.0, 0.0])]\n",
    "\n",
    "    X, Y, words = FeatureGeneration(train_preprocessed_df, morbidity).tf_idf()\n",
    "\n",
    "    if len(collections.Counter(list(Y)).keys()) >=2:\n",
    "        smote = SMOTE(random_state=42,k_neighbors=2)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X, Y)\n",
    "        X, Y =  X_train_resampled, y_train_resampled\n",
    "        \n",
    "        # add KFold cross validation\n",
    "        skf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "        f1_macro_list = []\n",
    "        f1_micro_list = []\n",
    "        for train_idx, val_idx in skf.split(X, Y):\n",
    "            X_train_fold, Y_train_fold = X[train_idx], Y[train_idx]\n",
    "            X_val_fold, Y_val_fold = X[val_idx], Y[val_idx]\n",
    "\n",
    "            # Training RF using TF-IDF Representation\n",
    "            dt_obj = DecisionTree(X_train_fold, Y_train_fold, X_val_fold, Y_val_fold, 100)\n",
    "            dt_obj.feature_selection_ExtraTreesClassifier()\n",
    "            dt_obj.train()\n",
    "\n",
    "            f1_macro, f1_micro = dt_obj.test_and_evaluate()\n",
    "\n",
    "            f1_macro_list.append(f1_macro)\n",
    "            f1_micro_list.append(f1_micro)\n",
    "\n",
    "        f1_macro = np.mean(f1_macro_list)\n",
    "        f1_micro = np.mean(f1_micro_list)\n",
    "    else:\n",
    "        f1_macro = 1\n",
    "        f1_micro = 1\n",
    "    \n",
    "    print(f\"{morbidity} == Macro F1 score: {f1_macro} and Micro F1 Score {f1_micro}\")\n",
    "\n",
    "    row_heading = morbidity\n",
    "\n",
    "    # data to be written to the CSV file\n",
    "    data = [f1_macro, f1_micro]\n",
    "    all_f1_macro_scores.append(f1_macro)\n",
    "    all_f1_micro_scores.append(f1_micro)\n",
    "\n",
    "    with open(\"./results/tf-idf/performance_DT_ExtraTreesClassifier.csv\", \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        row = [row_heading]\n",
    "        row.extend(data)\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "with open(\"./results/tf-idf/performance_DT_ExtraTreesClassifier.csv\", \"a\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    row = [\"Overall-Average\"]\n",
    "    row.extend([sum(all_f1_macro_scores)/len(all_f1_macro_scores),  sum(all_f1_micro_scores)/len(all_f1_micro_scores) ])\n",
    "    writer.writerow(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(572, 600) (572,) Counter({0.0: 502, 1.0: 70})\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m# Training RF using TF-IDF Representation\u001b[39;00m\n\u001b[1;32m     33\u001b[0m dt_obj \u001b[39m=\u001b[39m DecisionTree(X_train_fold, Y_train_fold, X_val_fold, Y_val_fold, \u001b[39m100\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m dt_obj\u001b[39m.\u001b[39;49mfeature_selection_InfoGainAttributeEval(morbidity\u001b[39m=\u001b[39;49mmorbidity)\n\u001b[1;32m     35\u001b[0m \u001b[39m# dt_obj.feature_selection_InfoGainAttributeEval(morbidity)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m dt_obj\u001b[39m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[39], line 25\u001b[0m, in \u001b[0;36mDecisionTree.feature_selection_InfoGainAttributeEval\u001b[0;34m(self, morbidity)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeature_selection_InfoGainAttributeEval\u001b[39m(\u001b[39mself\u001b[39m, morbidity):\n\u001b[0;32m---> 25\u001b[0m     loader \u001b[39m=\u001b[39m Loader(classname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mweka.core.converters.ArffLoader\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     26\u001b[0m     train_data \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mload_file(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./dataset/train/train_\u001b[39m\u001b[39m{\u001b[39;00mmorbidity\u001b[39m}\u001b[39;00m\u001b[39m_tfidf.arff\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m     train_data\u001b[39m.\u001b[39mclass_is_last()\n",
      "File \u001b[0;32m~/Documents/Classes/CS598DLH/.conda/lib/python3.10/site-packages/weka/core/converters.py:42\u001b[0m, in \u001b[0;36mLoader.__init__\u001b[0;34m(self, classname, jobject, options)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mInitializes the specified loader either using the classname or the JB_Object.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m:type options: list\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m jobject \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     jobject \u001b[39m=\u001b[39m Loader\u001b[39m.\u001b[39;49mnew_instance(classname)\n\u001b[1;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menforce_type(jobject, \u001b[39m\"\u001b[39m\u001b[39mweka.core.converters.Loader\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[39msuper\u001b[39m(Loader, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(jobject\u001b[39m=\u001b[39mjobject, options\u001b[39m=\u001b[39moptions)\n",
      "File \u001b[0;32m~/Documents/Classes/CS598DLH/.conda/lib/python3.10/site-packages/weka/core/classes.py:614\u001b[0m, in \u001b[0;36mJavaObject.new_instance\u001b[0;34m(cls, classname, options)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[39mif\u001b[39;00m options \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    610\u001b[0m         options \u001b[39m=\u001b[39m []\n\u001b[1;32m    611\u001b[0m     \u001b[39mreturn\u001b[39;00m javabridge\u001b[39m.\u001b[39mstatic_call(\n\u001b[1;32m    612\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLweka/core/Utils;\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mforName\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    613\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(Ljava/lang/Class;Ljava/lang/String;[Ljava/lang/String;)Ljava/lang/Object;\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m--> 614\u001b[0m         javabridge\u001b[39m.\u001b[39;49mclass_for_name(\u001b[39m\"\u001b[39;49m\u001b[39mjava.lang.Object\u001b[39;49m\u001b[39m\"\u001b[39;49m), classname, options)\n\u001b[1;32m    615\u001b[0m \u001b[39mexcept\u001b[39;00m JavaException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    616\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFailed to instantiate \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m classname \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/Documents/Classes/CS598DLH/.conda/lib/python3.10/site-packages/javabridge/jutil.py:1743\u001b[0m, in \u001b[0;36mclass_for_name\u001b[0;34m(classname, ldr)\u001b[0m\n\u001b[1;32m   1737\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Return a ``java.lang.Class`` for the given name.\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[39m:param classname: the class name in dotted form, e.g. \"java.lang.String\"\u001b[39;00m\n\u001b[1;32m   1740\u001b[0m \n\u001b[1;32m   1741\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m ldr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 1743\u001b[0m     ldr \u001b[39m=\u001b[39m static_call(\u001b[39m'\u001b[39;49m\u001b[39mjava/lang/ClassLoader\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mgetSystemClassLoader\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m   1744\u001b[0m                       \u001b[39m'\u001b[39;49m\u001b[39m()Ljava/lang/ClassLoader;\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m   1745\u001b[0m \u001b[39mreturn\u001b[39;00m static_call(\u001b[39m'\u001b[39m\u001b[39mjava/lang/Class\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mforName\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m   1746\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39m(Ljava/lang/String;ZLjava/lang/ClassLoader;)\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1747\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mLjava/lang/Class;\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m   1748\u001b[0m                    classname, \u001b[39mTrue\u001b[39;00m, ldr)\n",
      "File \u001b[0;32m~/Documents/Classes/CS598DLH/.conda/lib/python3.10/site-packages/javabridge/jutil.py:939\u001b[0m, in \u001b[0;36mstatic_call\u001b[0;34m(class_name, method_name, sig, *args)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Call a static method on a class\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[39m:param class_name: name of the class, using slashes\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    936\u001b[0m \n\u001b[1;32m    937\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    938\u001b[0m env \u001b[39m=\u001b[39m get_env()\n\u001b[0;32m--> 939\u001b[0m fn \u001b[39m=\u001b[39m make_static_call(class_name, method_name, sig)\n\u001b[1;32m    940\u001b[0m args_sig \u001b[39m=\u001b[39m split_sig(sig[\u001b[39m1\u001b[39m:sig\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)])\n\u001b[1;32m    941\u001b[0m ret_sig \u001b[39m=\u001b[39m sig[sig\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/Documents/Classes/CS598DLH/.conda/lib/python3.10/site-packages/javabridge/jutil.py:910\u001b[0m, in \u001b[0;36mmake_static_call\u001b[0;34m(class_name, method_name, sig)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Create a function that performs a call of a static method\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[39mmake_static_call produces a function that is faster than static_call\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    907\u001b[0m \n\u001b[1;32m    908\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    909\u001b[0m env \u001b[39m=\u001b[39m get_env()\n\u001b[0;32m--> 910\u001b[0m klass \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mfind_class(class_name)\n\u001b[1;32m    911\u001b[0m \u001b[39mif\u001b[39;00m klass \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    912\u001b[0m     jexception \u001b[39m=\u001b[39m get_env()\u001b[39m.\u001b[39mexception_occurred()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_class'"
     ]
    }
   ],
   "source": [
    "with open(\"./results/tf-idf/performance_DT_InfoGain.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([column_headings[0], column_headings[1], column_headings[2]])\n",
    "    \n",
    "all_f1_macro_scores = []\n",
    "all_f1_micro_scores = []\n",
    "\n",
    "for morbidity in morbidities:\n",
    "    train_preprocessed_df = pd.read_csv('./dataset/train/train_intuitive_preprocessed.csv')\n",
    "    train_preprocessed_df = train_preprocessed_df[train_preprocessed_df[morbidity].isin([1.0, 0.0])]\n",
    "\n",
    "    X, Y, words = FeatureGeneration(train_preprocessed_df, morbidity).tf_idf()\n",
    "\n",
    "    \n",
    "    if len(collections.Counter(list(Y)).keys()) < 2:\n",
    "        f1_macro = 1\n",
    "        f1_micro = 1\n",
    "    else:\n",
    "        smote = SMOTE(random_state=42,k_neighbors=2)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X, Y)\n",
    "        X, Y =  X_train_resampled, y_train_resampled\n",
    "        \n",
    "        # add KFold cross validation\n",
    "        skf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "        f1_macro_list = []\n",
    "        f1_micro_list = []\n",
    "        for train_idx, val_idx in skf.split(X, Y):\n",
    "            X_train_fold, Y_train_fold = X[train_idx], Y[train_idx]\n",
    "            X_val_fold, Y_val_fold = X[val_idx], Y[val_idx]\n",
    "\n",
    "            # Training RF using TF-IDF Representation\n",
    "            dt_obj = DecisionTree(X_train_fold, Y_train_fold, X_val_fold, Y_val_fold, 100)\n",
    "            dt_obj.feature_selection_InfoGainAttributeEval(morbidity=morbidity)\n",
    "            # dt_obj.feature_selection_InfoGainAttributeEval(morbidity)\n",
    "            dt_obj.train()\n",
    "\n",
    "            f1_macro, f1_micro = dt_obj.test_and_evaluate()\n",
    "\n",
    "            f1_macro_list.append(f1_macro)\n",
    "            f1_micro_list.append(f1_micro)\n",
    "\n",
    "        f1_macro = np.mean(f1_macro_list)\n",
    "        f1_micro = np.mean(f1_micro_list)\n",
    "    print(f\"{morbidity} == Macro F1 score: {f1_macro} and Micro F1 Score {f1_micro}\")\n",
    "\n",
    "    row_heading = morbidity\n",
    "\n",
    "    # data to be written to the CSV file\n",
    "    data = [f1_macro, f1_micro]\n",
    "    all_f1_macro_scores.append(f1_macro)\n",
    "    all_f1_micro_scores.append(f1_micro)\n",
    "\n",
    "    with open(\"./results/tf-idf/performance_DT_InfoGain.csv\", \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        row = [row_heading]\n",
    "        row.extend(data)\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "with open(\"./results/tf-idf/performance_DT_InfoGain.csv\", \"a\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    row = [\"Overall-Average\"]\n",
    "    row.extend([sum(all_f1_macro_scores)/len(all_f1_macro_scores),  sum(all_f1_micro_scores)/len(all_f1_micro_scores) ])\n",
    "    writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
